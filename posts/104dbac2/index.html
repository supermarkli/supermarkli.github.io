<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/video-game.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/video-game.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/video-game.ico">
  <link rel="mask-icon" href="/images/video-game.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"supermarkli.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"agate","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"show_result":true,"style":"default"},"fold":{"enable":false,"height":500}},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="选择合适的优化器以及理解其背后的参数，往往决定了模型是能够快速收敛到最佳状态，还是在训练过程中产生“梯度爆炸”甚至完全无法收敛。本文将通俗地解释这些概念，并深入剖析 PyTorch 中常用优化器的关键参数。">
<meta property="og:type" content="article">
<meta property="og:title" content="优化器详解：从SGD到Adam的原理与参数调优">
<meta property="og:url" content="https://supermarkli.github.io/posts/104dbac2/index.html">
<meta property="og:site_name" content="吃糠咽菜">
<meta property="og:description" content="选择合适的优化器以及理解其背后的参数，往往决定了模型是能够快速收敛到最佳状态，还是在训练过程中产生“梯度爆炸”甚至完全无法收敛。本文将通俗地解释这些概念，并深入剖析 PyTorch 中常用优化器的关键参数。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-12-02T15:50:07.000Z">
<meta property="article:modified_time" content="2025-12-03T01:53:28.604Z">
<meta property="article:author" content="吃糠咽菜">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="优化器">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://supermarkli.github.io/posts/104dbac2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://supermarkli.github.io/posts/104dbac2/","path":"posts/104dbac2/","title":"优化器详解：从SGD到Adam的原理与参数调优"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>优化器详解：从SGD到Adam的原理与参数调优 | 吃糠咽菜</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.7.0/mermaid.min.js","integrity":"sha256-4+IKDqhZ/sXjc8Wtl2/MsxI4e0s1KpEVdbEP7V/Lz8U="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>



  <script src="/js/third-party/pace.js" defer></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":false,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="吃糠咽菜" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">吃糠咽菜</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">好记性不如烂笔头</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%E8%92%99%E7%9C%BC%E4%B8%8B%E5%B1%B1"><span class="nav-number">1.</span> <span class="nav-text">1. 通俗理解：蒙眼下山</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E9%80%89%E6%89%8Bsgd-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.1.</span> <span class="nav-text">🚶 基础选手：SGD (随机梯度下降)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9B%E9%98%B6%E9%80%89%E6%89%8Badam-%E8%87%AA%E9%80%82%E5%BA%94%E7%9F%A9%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.2.</span> <span class="nav-text">🏎️ 进阶选手：Adam (自适应矩估计)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0%E4%B8%8E%E5%90%AB%E4%B9%89"><span class="nav-number">2.</span> <span class="nav-text">2. 深入剖析：关键参数与含义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd-%E7%9A%84%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">SGD 的关键参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam-%E7%9A%84%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">Adam 的关键参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E9%81%BF%E5%9D%91%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">3.</span> <span class="nav-text">3. 实战避坑：为什么会“梯度爆炸”？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B8%E5%9E%8B%E4%BA%8B%E6%95%85%E7%8E%B0%E5%9C%BA"><span class="nav-number">3.1.</span> <span class="nav-text">典型事故现场</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%89%E5%9E%8B%E6%8C%87%E5%8D%97%E6%88%91%E8%AF%A5%E7%94%A8%E5%93%AA%E4%B8%AA"><span class="nav-number">4.</span> <span class="nav-text">4. 选型指南：我该用哪个？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9-sgd-%E7%9A%84%E5%9C%BA%E6%99%AF"><span class="nav-number">4.1.</span> <span class="nav-text">✅ 选择 SGD 的场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9-adam-%E7%9A%84%E5%9C%BA%E6%99%AF"><span class="nav-number">4.2.</span> <span class="nav-text">✅ 选择 Adam 的场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E8%B4%B4%E5%A3%ABadamw-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">4.3.</span> <span class="nav-text">💡 小贴士：AdamW 是什么？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%93%E5%B1%95%E5%85%B6%E4%BB%96%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%80%E8%A7%88"><span class="nav-number">6.</span> <span class="nav-text">5. 拓展：其他常见优化器一览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rmsprop-root-mean-square-propagation"><span class="nav-number">6.1.</span> <span class="nav-text">📊 RMSprop (Root Mean Square Propagation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adagrad-adaptive-gradient"><span class="nav-number">6.2.</span> <span class="nav-text">📈 AdaGrad (Adaptive Gradient)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adadelta"><span class="nav-number">6.3.</span> <span class="nav-text">🔄 AdaDelta</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adamw-adam-with-weight-decay-fix"><span class="nav-number">6.4.</span> <span class="nav-text">⚡ AdamW (Adam with Weight Decay Fix)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%96%B0%E5%85%B4%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">6.5.</span> <span class="nav-text">🎯 其他新兴优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nadam-nesterov-accelerated-adam"><span class="nav-number">6.5.1.</span> <span class="nav-text">NAdam (Nesterov-accelerated Adam)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#radam-rectified-adam"><span class="nav-number">6.5.2.</span> <span class="nav-text">RAdam (Rectified Adam)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lookahead"><span class="nav-number">6.5.3.</span> <span class="nav-text">Lookahead</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E5%AF%B9%E6%AF%94%E8%A1%A8"><span class="nav-number">6.6.</span> <span class="nav-text">📋 快速对比表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E5%BB%BA%E8%AE%AE"><span class="nav-number">6.7.</span> <span class="nav-text">💡 实用建议</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="吃糠咽菜"
      src="/images/csnr.jpg">
  <p class="site-author-name" itemprop="name">吃糠咽菜</p>
  <div class="site-description" itemprop="description">一些乱七八糟的笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">91</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/supermarkli" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;supermarkli" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:12110504@mail.sustech.edu.cn" title="E-Mail → mailto:12110504@mail.sustech.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://supermarkli.github.io/posts/104dbac2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/csnr.jpg">
      <meta itemprop="name" content="吃糠咽菜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吃糠咽菜">
      <meta itemprop="description" content="一些乱七八糟的笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="优化器详解：从SGD到Adam的原理与参数调优 | 吃糠咽菜">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          优化器详解：从SGD到Adam的原理与参数调优
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-12-02 23:50:07" itemprop="dateCreated datePublished" datetime="2025-12-02T23:50:07+08:00">2025-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-12-03 09:53:28" itemprop="dateModified" datetime="2025-12-03T09:53:28+08:00">2025-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">深度学习基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>选择合适的优化器以及理解其背后的参数，往往决定了模型是能够快速收敛到最佳状态，还是在训练过程中产生“梯度爆炸”甚至完全无法收敛。本文将通俗地解释这些概念，并深入剖析 PyTorch 中常用优化器的关键参数。</p>
<span id="more"></span>
<h2 id="通俗理解蒙眼下山">1. 通俗理解：蒙眼下山</h2>
<p>想象你被蒙住双眼，放置在一座高山上。你的目标是下到山谷的最低点——在深度学习中，这代表着让模型的<strong>损失函数 (Loss)</strong> 最小化。</p>
<p>优化器，就是你的<strong>下山策略</strong>。</p>
<pre>
<code class="mermaid">

graph TD
    Start(&quot;山顶 (高 Loss)&quot;) --&gt; Strategy{选择优化器}
    Strategy -- &quot;SGD&quot; --&gt; Path1(&quot;固定步长，稳步试探&quot;)
    Strategy -- &quot;Adam&quot; --&gt; Path2(&quot;自适应速度，智能调整&quot;)
    Path1 --&gt; Goal(&quot;山谷 (低 Loss)&quot;)
    Path2 --&gt; Goal
</code>
</pre>
<h3 id="基础选手sgd-随机梯度下降">🚶 基础选手：SGD (随机梯度下降)</h3>
<p><strong>策略</strong>：用脚探一下当前的地面，感觉哪个方向是向下的，就往那个方向迈出固定的一步。</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>老实</strong>：它完全依赖当前的梯度方向。你设定多大的步长（学习率），它就走多远。</li>
<li><strong>稳健但缓慢</strong>：如果步长太小，下山速度极慢；如果步长太大，可能会在山谷震荡甚至跑偏。</li>
<li><strong>优势</strong>：在计算机视觉（CV）任务中（如 ResNet 训练），SGD 往往能找到更平坦、泛化能力更强的极小值点。</li>
</ul>
<h3 id="进阶选手adam-自适应矩估计">🏎️ 进阶选手：Adam (自适应矩估计)</h3>
<p><strong>策略</strong>：它不仅看当前的坡度，还"记得"之前的速度（动量），并且会根据地形的复杂程度自动调整步子大小。</p>
<ul>
<li><strong>平坦路段</strong>：自动加速。</li>
<li><strong>陡峭悬崖</strong>：自动减速，防止冲过头。</li>
</ul>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>智能</strong>：结合了动量（Momentum）和自适应学习率（RMSprop 的思想）。</li>
<li><strong>快速</strong>：收敛速度通常远快于 SGD。</li>
<li><strong>劣势</strong>：对“初始推力”（学习率）非常敏感，且在某些 CV 任务上，最终的泛化性能可能不如精心调教的 SGD。</li>
</ul>
<hr />
<h2 id="深入剖析关键参数与含义">2. 深入剖析：关键参数与含义</h2>
<p>在 PyTorch 中，我们通常通过 <code>torch.optim</code> 来调用这些优化器。理解每个参数的物理含义，是避免“炼丹”失败的关键。</p>
<h3 id="sgd-的关键参数">SGD 的关键参数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong><code>lr</code> (Learning Rate, 学习率)</strong>
<ul>
<li><strong>含义</strong>：下山的“步长”。</li>
<li><strong>通俗解释</strong>：决定了你每一次更新参数时变化有多大。</li>
<li><strong>典型值</strong>：<code>0.1</code>, <code>0.01</code>。SGD 需要较大的 <code>lr</code> 才能推动模型前进。</li>
</ul></li>
<li><strong><code>momentum</code> (动量)</strong>
<ul>
<li><strong>含义</strong>：模拟物理中的“惯性”。</li>
<li><strong>通俗解释</strong>：如果之前一直是下坡，那么这次不仅要看当前的坡度，还要加上之前的速度冲下去。这有助于冲过局部的平坦区域（鞍点）或小坑。</li>
<li><strong>典型值</strong>：<code>0.9</code>。这意味着当前的更新方向有 90% 来自之前的积累，10% 来自当前的梯度。</li>
</ul></li>
<li><strong><code>weight_decay</code> (权重衰减)</strong>
<ul>
<li><strong>含义</strong>：L2 正则化。</li>
<li><strong>通俗解释</strong>：一种“惩罚机制”。为了防止模型为了拟合数据而变得过于复杂（参数值过大），我们在损失函数里加了一项惩罚。它就像给模型加了“刹车”，防止过拟合。</li>
<li><strong>典型值</strong>：<code>1e-4</code> (0.0001) 或 <code>5e-4</code>。</li>
</ul></li>
</ol>
<h3 id="adam-的关键参数">Adam 的关键参数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-8</span>)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong><code>lr</code> (Learning Rate)</strong>
<ul>
<li><strong>注意</strong>：Adam 的学习率通常比 SGD 小得多。</li>
<li><strong>典型值</strong>：<code>0.001</code> (1e-3) 或 <code>3e-4</code>。这是因为 Adam 内部会自动根据梯度的一阶矩和二阶矩放大更新幅度。</li>
</ul></li>
<li><strong><code>betas</code> (Beta1, Beta2)</strong>
<ul>
<li><strong>含义</strong>：控制两个“记忆”的衰减率。</li>
<li><strong><code>beta1</code> (默认 0.9)</strong>：对应 <strong>一阶矩估计</strong>（即动量 Momentum）。控制“惯性”保持多久。</li>
<li><strong><code>beta2</code> (默认 0.999)</strong>：对应 <strong>二阶矩估计</strong>（即梯度的平方）。用来衡量“路况”的平稳程度，从而实现自适应步长。</li>
<li><strong>调参建议</strong>：通常保持默认即可。</li>
</ul></li>
<li><strong><code>eps</code> (Epsilon)</strong>
<ul>
<li><strong>含义</strong>：数值稳定性项。</li>
<li><strong>作用</strong>：防止分母为零。在计算自适应步长时，分母是梯度的平方根，加一个极小数 <code>eps</code> 避免除零错误。</li>
</ul></li>
</ol>
<hr />
<h2 id="实战避坑为什么会梯度爆炸">3. 实战避坑：为什么会“梯度爆炸”？</h2>
<p>在实际开发中，一个最常见的错误就是<strong>将 SGD 的经验直接套用到 Adam 上</strong>。</p>
<h3 id="典型事故现场">典型事故现场</h3>
<p>假设你习惯了训练 ResNet，配置了 <code>lr=0.1</code>。然后你想尝试一下 Transformer，于是把优化器改成了 Adam，但<strong>忘记了修改学习率</strong>。</p>
<pre>
<code class="mermaid">

graph TD
    Config[&quot;配置: Adam 优化器&quot;] --&gt; Input[&quot;输入: SGD 级别的学习率 (0.1)&quot;]
    Input -- &quot;对于 Adam 来说&quot; --&gt; Mechanism[&quot;Adam 内部的自适应放大机制&quot;]
    Mechanism --&gt; Result[&quot;实际更新步长过大 (相当于 SGD 的 100 倍)&quot;]
    Result --&gt; Boom[&quot;参数变为 NaN &#x2F; Infinity&quot;]
    Boom --&gt; Loss[&quot;Loss 震荡或不收敛&quot;]
</code>
</pre>
<p><strong>原因分析</strong>： Adam 自带“助推器”（自适应调整）。对于 Adam 来说，<code>0.001</code> 已经是很快的速度了。如果你给它 <code>0.1</code> 的初始推力，相当于给一辆法拉利加了火箭燃料，瞬间就会因为步长过大冲出赛道，导致参数变成 <code>NaN</code> (Not a Number) 或无穷大。</p>
<p><strong>修正方案</strong>： 一旦切换到 Adam，务必将学习率降低 1-2 个数量级（从 <code>0.1</code> 降到 <code>0.001</code>）。</p>
<hr />
<h2 id="选型指南我该用哪个">4. 选型指南：我该用哪个？</h2>
<p>没有绝对的“最好”，只有“最适合”。</p>
<h3 id="选择-sgd-的场景">✅ 选择 SGD 的场景</h3>
<ul>
<li><strong>任务</strong>：计算机视觉标准任务（图像分类、物体检测、分割）。</li>
<li><strong>代表模型</strong>：ResNet, VGG, YOLO。</li>
<li><strong>理由</strong>：虽然收敛慢，需要配合学习率衰减策略（Learning Rate Scheduler），但往往能收敛到更优的解，测试集准确率更高。</li>
<li><strong>推荐配置</strong>：<code>SGD(lr=0.1, momentum=0.9, weight_decay=5e-4)</code></li>
</ul>
<h3 id="选择-adam-的场景">✅ 选择 Adam 的场景</h3>
<ul>
<li><strong>任务</strong>：自然语言处理（NLP）、强化学习、生成对抗网络（GAN）、或快速原型开发。</li>
<li><strong>代表模型</strong>：Transformer, BERT, GPT, ViT。</li>
<li><strong>理由</strong>：收敛速度极快，对超参数（Hyper-parameters）不那么敏感，开箱即用。</li>
<li><strong>推荐配置</strong>：<code>Adam(lr=1e-3)</code> 或 <code>AdamW(lr=3e-4)</code></li>
</ul>
<h3 id="小贴士adamw-是什么">💡 小贴士：AdamW 是什么？</h3>
<p>你可能会在现在的代码中经常看到 <code>AdamW</code>。它是 Adam 的修正版。简单来说，Adam 在处理 <code>weight_decay</code>（权重衰减）时存在一个理论上的 Bug，<code>AdamW</code> 修复了这个问题，使得它在训练 Transformer 等大模型时效果更好、更稳定。</p>
<hr />
<h2 id="总结">总结</h2>
<ul>
<li><strong>SGD</strong> 像手动挡赛车，上限高，但需要精细的油门控制（调参）。</li>
<li><strong>Adam</strong> 像自动挡跑车，起步快，省心，但在某些赛道上可能不如手动挡精准。</li>
<li><strong>切记</strong>：换车（优化器）的时候，千万别忘了换挡（调整学习率）！</li>
</ul>
<hr />
<h2 id="拓展其他常见优化器一览">5. 拓展：其他常见优化器一览</h2>
<p>除了 SGD 和 Adam，深度学习工具箱中还有许多其他优化器。了解它们有助于在特定场景下做出更合适的选择。</p>
<h3 id="rmsprop-root-mean-square-propagation">📊 RMSprop (Root Mean Square Propagation)</h3>
<p><strong>诞生背景</strong>：为了解决 AdaGrad 学习率过早衰减的问题。</p>
<p><strong>核心思想</strong>：使用梯度的<strong>移动平均平方</strong>（而非累积平方）来调整学习率，这样可以让学习率在训练后期仍然保持活力。</p>
<p><strong>下山比喻</strong>：就像一个经验丰富的登山者，会根据最近一段路的情况（而不是整段路程）来判断下一步的步长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure>
<p><strong>关键参数</strong>： * <strong><code>lr</code></strong>：学习率，典型值 <code>0.01</code> * <strong><code>alpha</code></strong>：平滑常数，控制历史梯度的衰减速度，默认 <code>0.99</code></p>
<p><strong>适用场景</strong>：RNN（循环神经网络）训练，是 Adam 出现之前的主流选择。</p>
<hr />
<h3 id="adagrad-adaptive-gradient">📈 AdaGrad (Adaptive Gradient)</h3>
<p><strong>核心思想</strong>：为每个参数维护一个<strong>累积的梯度平方和</strong>，频繁更新的参数会获得更小的学习率，稀疏参数会获得更大的学习率。</p>
<p><strong>下山比喻</strong>：如果某个方向你经常走（梯度大），说明这个方向可能已经接近目标了，就放慢脚步；如果某个方向很少走（梯度小），就加快探索。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adagrad(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p><strong>特点</strong>： * <strong>优点</strong>：对稀疏梯度友好，适合处理稀疏数据。 * <strong>缺点</strong>：学习率会单调递减，训练后期可能过小导致无法继续学习。</p>
<p><strong>适用场景</strong>：自然语言处理中的稀疏特征学习，但现在已较少使用。</p>
<hr />
<h3 id="adadelta">🔄 AdaDelta</h3>
<p><strong>诞生背景</strong>：AdaGrad 的改进版，解决了学习率过早衰减的问题。</p>
<p><strong>核心创新</strong>：不需要手动设置学习率！它会根据参数更新的历史信息自动调整。</p>
<p><strong>下山比喻</strong>：完全自动驾驶，不仅会根据路况调整速度，还会根据之前走过的路来预测下一步该走多远。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adadelta(model.parameters(), rho=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><strong>关键参数</strong>： * <strong><code>rho</code></strong>：衰减率，控制历史信息的权重，默认 <code>0.9</code></p>
<p><strong>特点</strong>： * <strong>优点</strong>：无需设置学习率，对超参数不敏感。 * <strong>缺点</strong>：收敛速度可能较慢，实际使用中不如 Adam 受欢迎。</p>
<hr />
<h3 id="adamw-adam-with-weight-decay-fix">⚡ AdamW (Adam with Weight Decay Fix)</h3>
<p><strong>核心改进</strong>：修复了 Adam 在处理权重衰减（Weight Decay）时的理论缺陷。</p>
<p><strong>技术细节</strong>：在原始 Adam 中，<code>weight_decay</code> 被错误地应用到了梯度上，而不是直接应用到参数上。AdamW 将权重衰减与梯度更新<strong>解耦</strong>，使得正则化效果更符合 L2 正则化的原始意图。</p>
<p><strong>下山比喻</strong>：Adam 在刹车（权重衰减）时，刹车片和油门混在一起了；AdamW 把刹车系统独立出来，刹车更精准。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-4</span>, weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p><strong>关键参数</strong>： * <strong><code>lr</code></strong>：学习率，典型值 <code>3e-4</code>（比 Adam 稍小） * <strong><code>weight_decay</code></strong>：权重衰减，典型值 <code>0.01</code>（可以比 Adam 中设置得更大）</p>
<p><strong>适用场景</strong>： * <strong>Transformer 模型</strong>：BERT、GPT 等大模型的标准选择 * <strong>计算机视觉</strong>：ViT（Vision Transformer）等 * <strong>现代深度学习</strong>：几乎成为新项目的默认选择</p>
<p><strong>为什么现在更推荐 AdamW？</strong> 1. 理论更严谨：权重衰减的实现方式更符合数学原理 2. 泛化性能更好：在大多数任务上测试集表现更优 3. 超参数更稳定：对 <code>weight_decay</code> 的敏感性更低</p>
<hr />
<h3 id="其他新兴优化器">🎯 其他新兴优化器</h3>
<h4 id="nadam-nesterov-accelerated-adam">NAdam (Nesterov-accelerated Adam)</h4>
<p>结合了 Nesterov 动量和 Adam 的优点，在收敛速度和稳定性之间取得更好的平衡。</p>
<h4 id="radam-rectified-adam">RAdam (Rectified Adam)</h4>
<p>解决了 Adam 在训练初期可能不稳定的问题，通过动态调整学习率来保证训练的稳定性。</p>
<h4 id="lookahead">Lookahead</h4>
<p>一种<strong>元优化器</strong>，可以包装在任何优化器（如 Adam、SGD）外面，通过"向前看"的策略来稳定训练过程。</p>
<hr />
<h3 id="快速对比表">📋 快速对比表</h3>
<table>
<thead>
<tr class="header">
<th>优化器</th>
<th>学习率需求</th>
<th>收敛速度</th>
<th>超参数敏感性</th>
<th>主要应用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD</strong></td>
<td>较大 (0.1)</td>
<td>慢</td>
<td>高</td>
<td>CV 任务 (ResNet)</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>较小 (0.001)</td>
<td>快</td>
<td>中</td>
<td>NLP、GAN、快速原型</td>
</tr>
<tr class="odd">
<td><strong>AdamW</strong></td>
<td>较小 (3e-4)</td>
<td>快</td>
<td>低</td>
<td>Transformer、现代大模型</td>
</tr>
<tr class="even">
<td><strong>RMSprop</strong></td>
<td>中等 (0.01)</td>
<td>中</td>
<td>中</td>
<td>RNN（历史选择）</td>
</tr>
<tr class="odd">
<td><strong>AdaGrad</strong></td>
<td>中等 (0.01)</td>
<td>慢（后期）</td>
<td>低</td>
<td>稀疏数据（较少使用）</td>
</tr>
<tr class="even">
<td><strong>AdaDelta</strong></td>
<td>无需设置</td>
<td>中</td>
<td>低</td>
<td>理论研究（较少使用）</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="实用建议">💡 实用建议</h3>
<ol type="1">
<li><strong>新手入门</strong>：直接使用 <code>AdamW(lr=3e-4)</code>，这是 2024 年最稳妥的选择。</li>
<li><strong>CV 任务追求极致</strong>：尝试 <code>SGD(lr=0.1, momentum=0.9)</code> 配合学习率调度器。</li>
<li><strong>大模型训练</strong>：优先考虑 <code>AdamW</code>，配合 <code>weight_decay=0.1</code>。</li>
<li><strong>实验阶段</strong>：先用 AdamW 快速验证想法，再根据任务特点微调。</li>
</ol>
<p>记住：<strong>没有万能的优化器，只有最适合你任务的优化器</strong>。理解原理比盲目选择更重要！</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" rel="tag"># 优化器</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/b08764f6/" rel="prev" title="实模式（Real Mode）">
                  <i class="fa fa-angle-left"></i> 实模式（Real Mode）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/a9a69d93/" rel="next" title="KeyMapping：win上使用mac键位">
                  KeyMapping：win上使用mac键位 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa-solid fa-headphones"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">吃糠咽菜</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">233k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:03</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
